{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"multiprocessing_rl.ipynb","version":"0.3.2","provenance":[{"file_id":"1ZzNFMUUi923foaVsYb4YjPy4mjKtnOxb","timestamp":1541340664747},{"file_id":"1_1H5bjWKYBVKbbs-Kj83dsfuZieDNcFU","timestamp":1534416190212}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hyyN-2qyK_T2","colab_type":"text"},"source":["# Stable Baselines, a fork of OpenAI Baselines - Easy Multiprocessing\n","\n","Github Repo: [https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)\n","\n","Medium article: [https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82](https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82)\n","\n","## Install Dependencies and Stable Baselines Using Pip\n","\n","List of full dependencies can be found in the [README](https://github.com/hill-a/stable-baselines).\n","\n","```\n","\n","sudo apt-get update && sudo apt-get install cmake libopenmpi-dev zlib1g-dev\n","```\n","\n","\n","```\n","\n","pip install stable-baselines\n","```"]},{"cell_type":"code","metadata":{"id":"503Gi2076F7u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1734},"outputId":"d8e070df-dae8-4f95-f860-4a5d358d28f2","executionInfo":{"status":"ok","timestamp":1559603667625,"user_tz":420,"elapsed":16047,"user":{"displayName":"Mariano Phielipp","photoUrl":"https://lh5.googleusercontent.com/-4HI6MgZOzBA/AAAAAAAAAAI/AAAAAAAAAAA/rNlgZ1ECWg0/s64/photo.jpg","userId":"05778665983387453541"}}},"source":["!apt install swig cmake libopenmpi-dev zlib1g-dev\n","!pip install stable-baselines==2.1.1"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","cmake is already the newest version (3.10.2-1ubuntu2).\n","zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n","zlib1g-dev set to manually installed.\n","libopenmpi-dev is already the newest version (2.1.1-8).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-410\n","Use 'apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  swig3.0\n","Suggested packages:\n","  swig-doc swig-examples swig3.0-examples swig3.0-doc\n","The following NEW packages will be installed:\n","  swig swig3.0\n","0 upgraded, 2 newly installed, 0 to remove and 8 not upgraded.\n","Need to get 1,100 kB of archives.\n","After this operation, 5,822 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n","Fetched 1,100 kB in 1s (1,332 kB/s)\n","Selecting previously unselected package swig3.0.\n","(Reading database ... 130912 files and directories currently installed.)\n","Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n","Unpacking swig3.0 (3.0.12-1) ...\n","Selecting previously unselected package swig.\n","Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n","Unpacking swig (3.0.12-1) ...\n","Setting up swig3.0 (3.0.12-1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Setting up swig (3.0.12-1) ...\n","Collecting stable-baselines==2.1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/33/c105a465fc1e258b1b136f46e41ef437c92620de275eae63df203210e66c/stable_baselines-2.1.1-py3-none-any.whl (221kB)\n","\u001b[K     |████████████████████████████████| 225kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.1.1) (1.3.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.1.1) (0.9.0)\n","Requirement already satisfied: zmq in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.1.1) (0.0.0)\n","Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.1.1) (3.38.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.1.1) (0.2.9)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.1.1) (0.6.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.1.1) (7.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.1.1) (3.0.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.1.1) (0.24.2)\n","Requirement already satisfied: glob2 in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.1.1) (0.6)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.1.1) (3.4.5.20)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.1.1) (4.28.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.1.1) (1.16.4)\n","Requirement already satisfied: tensorflow>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.1.1) (1.13.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.1.1) (0.13.2)\n","Collecting gym[atari,classic_control]==0.10.5 (from stable-baselines==2.1.1)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/50/ed4a03d2be47ffd043be2ee514f329ce45d98a30fe2d1b9c61dea5a9d861/gym-0.10.5.tar.gz (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 43.1MB/s \n","\u001b[?25hRequirement already satisfied: mpi4py in /usr/local/lib/python3.6/dist-packages (from stable-baselines==2.1.1) (3.0.1)\n","Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from zmq->stable-baselines==2.1.1) (17.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from progressbar2->stable-baselines==2.1.1) (1.12.0)\n","Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->stable-baselines==2.1.1) (2.3.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines==2.1.1) (2.5.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines==2.1.1) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines==2.1.1) (1.1.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines==2.1.1) (2.4.0)\n","Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines==2.1.1) (2018.9)\n","Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines==2.1.1) (1.13.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines==2.1.1) (1.1.0)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines==2.1.1) (0.7.1)\n","Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines==2.1.1) (1.13.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines==2.1.1) (1.0.7)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines==2.1.1) (1.15.0)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines==2.1.1) (0.2.2)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines==2.1.1) (0.33.4)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines==2.1.1) (3.7.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines==2.1.1) (1.0.9)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines==2.1.1) (0.8.0)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]==0.10.5->stable-baselines==2.1.1) (2.21.0)\n","Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]==0.10.5->stable-baselines==2.1.1) (1.3.2)\n","Requirement already satisfied: atari_py>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]==0.10.5->stable-baselines==2.1.1) (0.1.15)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]==0.10.5->stable-baselines==2.1.1) (4.3.0)\n","Requirement already satisfied: PyOpenGL in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]==0.10.5->stable-baselines==2.1.1) (3.1.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->stable-baselines==2.1.1) (41.0.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow>=1.5.0->stable-baselines==2.1.1) (0.15.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow>=1.5.0->stable-baselines==2.1.1) (3.1.1)\n","Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow>=1.5.0->stable-baselines==2.1.1) (3.0.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow>=1.5.0->stable-baselines==2.1.1) (2.8.0)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari,classic_control]==0.10.5->stable-baselines==2.1.1) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari,classic_control]==0.10.5->stable-baselines==2.1.1) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari,classic_control]==0.10.5->stable-baselines==2.1.1) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari,classic_control]==0.10.5->stable-baselines==2.1.1) (2019.3.9)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[atari,classic_control]==0.10.5->stable-baselines==2.1.1) (0.16.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->gym[atari,classic_control]==0.10.5->stable-baselines==2.1.1) (0.46)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/cb/14/71/f4ab006b1e6ff75c2b54985c2f98d0644fffe9c1dddc670925\n","Successfully built gym\n","Installing collected packages: gym, stable-baselines\n","  Found existing installation: gym 0.10.11\n","    Uninstalling gym-0.10.11:\n","      Successfully uninstalled gym-0.10.11\n","  Found existing installation: stable-baselines 2.2.1\n","    Uninstalling stable-baselines-2.2.1:\n","      Successfully uninstalled stable-baselines-2.2.1\n","Successfully installed gym-0.10.5 stable-baselines-2.1.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FtY8FhliLsGm","colab_type":"text"},"source":["## Import policy, RL agent, ..."]},{"cell_type":"code","metadata":{"id":"BIedd7Pz9sOs","colab_type":"code","colab":{}},"source":["import time\n","\n","import gym\n","import numpy as np\n","\n","from stable_baselines.common.policies import MlpPolicy\n","from stable_baselines.common.vec_env import DummyVecEnv, SubprocVecEnv\n","from stable_baselines.common import set_global_seeds\n","from stable_baselines import ACKTR"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t5WNF6G5gWZ1","colab_type":"text"},"source":["## Multiprocessing RL Training\n","\n","To multiprocess RL training, we will just have to wrap the Gym env into a SubprocVecEnv object, that will take care of synchronising the processes. The idea is that each process will run an indepedent instance of the Gym env.\n","\n","For that, we need an additional utility function, `make_env`, that will instantiate the environments and make sure they are different (using different random seed)."]},{"cell_type":"code","metadata":{"id":"TgjfyOTPVxG6","colab_type":"code","colab":{}},"source":["def make_env(env_id, rank, seed=0):\n","    \"\"\"\n","    Utility function for multiprocessed env.\n","    \n","    :param env_id: (str) the environment ID\n","    :param num_env: (int) the number of environment you wish to have in subprocesses\n","    :param seed: (int) the inital seed for RNG\n","    :param rank: (int) index of the subprocess\n","    \"\"\"\n","    def _init():\n","        env = gym.make(env_id)\n","        env.seed(seed + rank)\n","        return env\n","    set_global_seeds(seed)\n","    return _init"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iPGIySi2g_RN","colab_type":"text"},"source":["The number of parallel process used is defined by the `num_cpu` variable.\n","\n","Because we use vectorized environment (SubprocVecEnv), the actions sent to the wrapped env must be an array (one action per process). Also, observations, rewards and dones are arrays."]},{"cell_type":"code","metadata":{"id":"pUWGZp3i9wyf","colab_type":"code","colab":{}},"source":["env_id = \"CartPole-v1\"\n","num_cpu = 4  # Number of processes to use\n","# Create the vectorized environment\n","env = SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])\n","\n","model = ACKTR(MlpPolicy, env, verbose=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4efFdrQ7MBvl","colab_type":"text"},"source":["We create a helper function to evaluate the agent:"]},{"cell_type":"code","metadata":{"id":"63M8mSKR-6Zt","colab_type":"code","colab":{}},"source":["def evaluate(model, num_steps=1000):\n","    \"\"\"\n","    Evaluate a RL agent\n","    :param model: (BaseRLModel object) the RL Agent\n","    :param num_steps: (int) number of timesteps to evaluate it\n","    :return: (float) Mean reward\n","    \"\"\"\n","    episode_rewards = [[0.0] for _ in range(env.num_envs)]\n","    obs = env.reset()\n","    for i in range(num_steps):\n","      # _states are only useful when using LSTM policies\n","      actions, _states = model.predict(obs)\n","      # here, action, rewards and dones are arrays\n","      # because we are using vectorized env\n","      obs, rewards, dones, info = env.step(actions)\n","      \n","      # Stats\n","      for i in range(env.num_envs):\n","          episode_rewards[i][-1] += rewards[i]\n","          if dones[i]:\n","              episode_rewards[i].append(0.0)\n","\n","    mean_rewards =  [0.0 for _ in range(env.num_envs)]\n","    n_episodes = 0\n","    for i in range(env.num_envs):\n","        mean_rewards[i] = np.mean(episode_rewards[i])     \n","        n_episodes += len(episode_rewards[i])   \n","\n","    # Compute mean reward\n","    mean_reward = round(np.mean(mean_rewards), 1)\n","    print(\"Mean reward:\", mean_reward, \"Num episodes:\", n_episodes)\n","\n","    return mean_reward\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjEVOIY8NVeK","colab_type":"text"},"source":["Let's evaluate the un-trained agent, this should be a random agent."]},{"cell_type":"code","metadata":{"id":"xDHLMA6NFk95","colab_type":"code","outputId":"8f5991f4-83eb-40db-dd8c-11406a4b12f0","executionInfo":{"status":"ok","timestamp":1559603675097,"user_tz":420,"elapsed":23466,"user":{"displayName":"Mariano Phielipp","photoUrl":"https://lh5.googleusercontent.com/-4HI6MgZOzBA/AAAAAAAAAAI/AAAAAAAAAAA/rNlgZ1ECWg0/s64/photo.jpg","userId":"05778665983387453541"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Random Agent, before training\n","mean_reward_before_train = evaluate(model, num_steps=1000)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Mean reward: 21.7 Num episodes: 185\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r5UoXTZPNdFE","colab_type":"text"},"source":["## Multiprocess VS Single Process Training\n","\n","Here, we will compare time taken using one vs 4 processes, it should take ~30s in total."]},{"cell_type":"code","metadata":{"id":"e4cfSXIB-pTF","colab_type":"code","colab":{}},"source":["n_timesteps = 25000\n","\n","# Multiprocessed RL Training\n","start_time = time.time()\n","model.learn(n_timesteps)\n","total_time_multi = time.time() - start_time\n","\n","print(\"Took {:.2f}s for multiprocessed version - {:.2f} FPS\".format(total_time_multi, n_timesteps / total_time_multi))\n","\n","# Single Process RL Training\n","single_process_model = ACKTR(MlpPolicy, DummyVecEnv([lambda: gym.make(env_id)]), verbose=0)\n","\n","start_time = time.time()\n","single_process_model.learn(n_timesteps)\n","total_time_single = time.time() - start_time\n","\n","print(\"Took {:.2f}s for single process version - {:.2f} FPS\".format(total_time_single, n_timesteps / total_time_single))\n","\n","print(\"Multiprocessed training is {:.2f}x faster!\".format(total_time_single / total_time_multi))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ygl_gVmV_QP7","colab_type":"code","outputId":"bddd5345-870d-4135-a782-931616554724","executionInfo":{"status":"ok","timestamp":1559603725975,"user_tz":420,"elapsed":74308,"user":{"displayName":"Mariano Phielipp","photoUrl":"https://lh5.googleusercontent.com/-4HI6MgZOzBA/AAAAAAAAAAI/AAAAAAAAAAA/rNlgZ1ECWg0/s64/photo.jpg","userId":"05778665983387453541"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Evaluate the trained agent\n","mean_reward = evaluate(model, num_steps=10000)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Mean reward: 226.7 Num episodes: 177\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QkWsoZ8emt0e","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}